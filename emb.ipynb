{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a99371-f88e-4b35-b3a5-e56406d1ad8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species individual_id\n",
       "0  00021adfb725ed.jpg  melon_headed_whale  cadddb1636b9\n",
       "1  000562241d384d.jpg      humpback_whale  1a71fbb72250\n",
       "2  0007c33415ce37.jpg  false_killer_whale  60008f293a2b\n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin  4b00fe572063\n",
       "4  00087baf5cef7a.jpg      humpback_whale  8e5253662392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000110707af0ba.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006287ec424cb.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000809ecb2ccad.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00098d1376dab2.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000b8d89c738bd.jpg</td>\n",
       "      <td>37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                        predictions\n",
       "0  000110707af0ba.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...\n",
       "1  0006287ec424cb.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...\n",
       "2  000809ecb2ccad.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...\n",
       "3  00098d1376dab2.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96...\n",
       "4  000b8d89c738bd.jpg  37c7aba965a5 114207cab555 a6e325d8e924 19fbb96..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15587\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from models.ch_mdl_dolg_efficientnet import ArcFaceLoss\n",
    "from models.eff import get_model\n",
    "train = pd.read_csv('../whale/train.csv')\n",
    "test = pd.read_csv('../whale/sample_submission.csv')\n",
    "classes = list(set(train.individual_id))\n",
    "classes_species = list(set(train.species))\n",
    "sorted(classes)\n",
    "sorted(classes_species)\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "print(len(classes))\n",
    "print(len(classes_species))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca47c8-a249-4174-be1f-046d80c80fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "from joblib import Parallel, delayed\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "def cos_similarity_matrix(a, b, eps=1e-8):\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    return sim_mt\n",
    "def get_topk_cossim(test_emb, tr_emb, batchsize = 64, k=10, device='cuda:0',verbose=True):\n",
    "    tr_emb = torch.tensor(tr_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    test_emb = torch.tensor(test_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    vals = []\n",
    "    inds = []\n",
    "    for test_batch in tqdm(test_emb.split(batchsize),disable=1-verbose):\n",
    "        sim_mat = cos_similarity_matrix(test_batch, tr_emb)\n",
    "        vals_batch, inds_batch = torch.topk(sim_mat, k=k, dim=1)\n",
    "        vals += [vals_batch.detach().cpu()]\n",
    "        inds += [inds_batch.detach().cpu()]\n",
    "    vals = torch.cat(vals)\n",
    "    inds = torch.cat(inds)\n",
    "    return vals, inds\n",
    "def get_topk_cossim_sub(test_emb, tr_emb, vals_x=None, batchsize = 64, k=10, device='cuda:0',verbose=True):\n",
    "    tr_emb = torch.tensor(tr_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    test_emb = torch.tensor(test_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    #vals_x = torch.tensor(vals_x, dtype = torch.float32, device=torch.device(device))\n",
    "    vals = []\n",
    "    inds = []\n",
    "    for test_batch in tqdm(test_emb.split(batchsize),disable=1-verbose):\n",
    "        sim_mat = cos_similarity_matrix(test_batch, tr_emb)\n",
    "        # sim_mat = torch.clamp(sim_mat,0,1) #- vals_x.repeat(sim_mat.shape[0], 1)\n",
    "        \n",
    "        vals_batch, inds_batch = torch.topk(sim_mat, k=k, dim=1)\n",
    "        vals += [vals_batch.detach().cpu()]\n",
    "        inds += [inds_batch.detach().cpu()]\n",
    "    vals = torch.cat(vals)\n",
    "    inds = torch.cat(inds)\n",
    "    return vals, inds\n",
    "def map_per_image(label, predictions):\n",
    "    indexes = np.unique(predictions, return_index=True)[1]\n",
    "    predictions = [predictions[index] for index in sorted(indexes)]\n",
    "    t = np.where(predictions[:5]==label)[0]\n",
    "    if len(t)>0:\n",
    "        return 1 / (t[0] + 1)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc48a4a8-6721-40a4-9d11-95621c9a5079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>classes</th>\n",
       "      <th>classes_species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>12348</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "      <td>1636</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "      <td>5842</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "      <td>4551</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "      <td>8721</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species individual_id  classes  \\\n",
       "0  00021adfb725ed.jpg  melon_headed_whale  cadddb1636b9    12348   \n",
       "1  000562241d384d.jpg      humpback_whale  1a71fbb72250     1636   \n",
       "2  0007c33415ce37.jpg  false_killer_whale  60008f293a2b     5842   \n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin  4b00fe572063     4551   \n",
       "4  00087baf5cef7a.jpg      humpback_whale  8e5253662392     8721   \n",
       "\n",
       "   classes_species  \n",
       "0               18  \n",
       "1               14  \n",
       "2                9  \n",
       "3                2  \n",
       "4               14  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['classes'] = train.groupby('individual_id').ngroup()\n",
    "train['classes_species'] = train.groupby('species').ngroup()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b5658c-f579-4364-8f5f-59866e2d1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "TRAIN_PATH = '../whale/crop/train_images'\n",
    "TEST_PATH = '../whale/crop/test_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9ece72-8ef4-422f-9d4a-24297e037cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug=False\n",
    "    apex=False\n",
    "    print_freq=100\n",
    "    num_workers=8\n",
    "    model_name='tf_efficientnet_b5_ns'\n",
    "    size=448\n",
    "    scheduler='CosineAnnealingWarmRestarts' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    epochs=15\n",
    "    #factor=0.2 # ReduceLROnPlateau\n",
    "    #patience=4 # ReduceLROnPlateau\n",
    "    #eps=1e-6 # ReduceLROnPlateau\n",
    "    #T_max=10 # CosineAnnealingLR\n",
    "    T_0=10 # CosineAnnealingWarmRestarts\n",
    "    lr=1e-4\n",
    "    min_lr=1e-6\n",
    "    batch_size=128\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    target_size=6\n",
    "    target_col='classes'\n",
    "    target_col2 = 'classes_species'\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    inference=False\n",
    "model_name = CFG.model_name\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1\n",
    "    train = train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfaa3d37-a2be-4495-817b-e0a5a124b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD,AdamW\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose\n",
    "    )\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import timm\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if CFG.apex:\n",
    "    from apex import amp\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae47e891-15c7-4bb3-b72e-1860d0edfc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  classes\n",
      "0     4          1\n",
      "      10         2\n",
      "      13         1\n",
      "      18         1\n",
      "      19         1\n",
      "                ..\n",
      "4     15575      1\n",
      "      15577      1\n",
      "      15578      6\n",
      "      15583      1\n",
      "      15584      1\n",
      "Length: 29343, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[CFG.target_col])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "print(folds.groupby(['fold', CFG.target_col]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a7169d5-5ecb-4c0e-8822-d5778638f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image'].values\n",
    "        self.labels = df['classes'].values\n",
    "        self.labels_s = df['classes_species'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{TRAIN_PATH}/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        label_species = torch.tensor(self.labels_s[idx]).long()\n",
    "        return image, label,label_species\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['image'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{TEST_PATH}/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0115cbe2-e694-4f5c-92a5-c67305880b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Transforms\n",
    "# ====================================================\n",
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0986d06-51ad-4199-b99d-33282b233701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, device):\n",
    "    model.eval()\n",
    "    emb = []\n",
    "    targets = []\n",
    "    start = end = time.time()\n",
    "    for step, (images, labels,labels2) in enumerate(tqdm(valid_loader)):\n",
    "        # measure data loading time\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        labels2 = labels2.to(device).long()\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model.extract_feature(images).cpu().numpy()\n",
    "            emb.append(y_preds)\n",
    "            targets.append(labels.cpu().numpy())\n",
    "    emb = np.concatenate(emb)\n",
    "    targets = np.concatenate(targets)\n",
    "    \n",
    "    return emb,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924fe6ca-7977-4245-b611-f3e78414b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_loop(folds, fold,run_cv = False):\n",
    "    \n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds, \n",
    "                                 transform=get_transforms(data='train'))\n",
    "    valid_dataset = TrainDataset(valid_folds, \n",
    "                                 transform=get_transforms(data='valid'))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=False, \n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=False, \n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    model = get_model(backbone_name = model_name)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')['model'])\n",
    "    emb_v,targets_v = valid_fn(valid_loader, model, device)\n",
    "    emb_t,targets_t = valid_fn(train_loader, model, device)\n",
    "    res = {}\n",
    "    res['emb_v'] = emb_v\n",
    "    res['targets_v'] = targets_v\n",
    "    res['emb_t'] = emb_t\n",
    "    res['targets_t'] = targets_t\n",
    "    if not run_cv: \n",
    "        return res\n",
    "    tr_embeddings = results[0]['emb_t']\n",
    "    val_embeddings = results[0]['emb_v']\n",
    "    targets = results[0]['targets_v']\n",
    "    targets_train = results[0]['targets_t']\n",
    "    EMB_SIZE = 512\n",
    "    vals_blend = []\n",
    "    labels_blend = []\n",
    "    inds_blend = []\n",
    "    for i in range(1):\n",
    "        vals, inds = get_topk_cossim_sub(val_embeddings[:,i*EMB_SIZE:(i+1)*EMB_SIZE], tr_embeddings[:,i*EMB_SIZE:(i+1)*EMB_SIZE], k=500)\n",
    "        vals = vals.data.cpu().numpy()\n",
    "        inds = inds.data.cpu().numpy()\n",
    "        labels = np.concatenate([targets_train[inds[:,i]].reshape(-1,1) for i in range(inds.shape[1])], axis=1)\n",
    "\n",
    "        vals_blend.append(vals)\n",
    "        labels_blend.append(labels)\n",
    "        inds_blend.append(inds)\n",
    "    vals = np.concatenate(vals_blend, axis=1)\n",
    "    inds = np.concatenate(inds_blend, axis=1)\n",
    "    labels = np.concatenate(labels_blend, axis=1)\n",
    "    M = []\n",
    "    for row in range(len(vals)):\n",
    "        m = map_per_image(targets[row],labels[row])\n",
    "        M.append(m)\n",
    "    return np.array(M).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "451bad14-194f-44d6-9ecb-1cb661c06544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:04<00:00,  1.23it/s]\n",
      "100%|██████████| 319/319 [03:59<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for fold in range(1,2):\n",
    "    results.append(infer_loop(folds, fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78b2140c-8413-470b-99ba-423fefcbfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_embeddings = results[0]['emb_t']\n",
    "val_embeddings = results[0]['emb_v']\n",
    "targets = results[0]['targets_v']\n",
    "targets_train = results[0]['targets_t']\n",
    "f = QuantileTransformer(output_distribution=\"normal\")\n",
    "f.fit(np.concatenate([val_embeddings],axis=0))\n",
    "tr_embeddings = f.transform(tr_embeddings)\n",
    "val_embeddings = f.transform(val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6624f6f-72c7-437d-95a3-6ceecf38bb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 942.45it/s]\n"
     ]
    }
   ],
   "source": [
    "EMB_SIZE = 512\n",
    "vals_blend = []\n",
    "labels_blend = []\n",
    "inds_blend = []\n",
    "for i in range(1):\n",
    "    \n",
    "    # vals_nl, inds_nl = get_topk_cossim(tr_embeddings[:,i*EMB_SIZE:(i+1)*EMB_SIZE], nonlandmark_embeddings[:,i*EMB_SIZE:(i+1)*EMB_SIZE], k=5)\n",
    "    # vals_nl = vals_nl[:,:].mean(axis=1).detach().cpu().numpy()\n",
    "    \n",
    "    vals, inds = get_topk_cossim_sub(val_embeddings[:,i*EMB_SIZE:(i+1)*EMB_SIZE], tr_embeddings[:,i*EMB_SIZE:(i+1)*EMB_SIZE], k=500)\n",
    "    vals = vals.data.cpu().numpy()\n",
    "    inds = inds.data.cpu().numpy()\n",
    "    labels = np.concatenate([targets_train[inds[:,i]].reshape(-1,1) for i in range(inds.shape[1])], axis=1)\n",
    "    \n",
    "    vals_blend.append(vals)\n",
    "    labels_blend.append(labels)\n",
    "    inds_blend.append(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9460b541-6170-4647-8dcf-04bd4d538ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.concatenate(vals_blend, axis=1)\n",
    "inds = np.concatenate(inds_blend, axis=1)\n",
    "labels = np.concatenate(labels_blend, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7a6776a-8021-4cc2-9755-3cc835c4cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5159090166878939\n"
     ]
    }
   ],
   "source": [
    "M = []\n",
    "for row in range(len(vals)):\n",
    "    m = map_per_image(targets[row],labels[row])\n",
    "    M.append(m)\n",
    "print(np.array(M).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a699378b-abda-4609-ab53-6309ef49d870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mmlab)",
   "language": "python",
   "name": "mmlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
